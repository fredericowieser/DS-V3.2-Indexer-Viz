# HUMAN-INSPIRED EPISODIC MEMORY FOR INFINITE CONTEXT LLMS

Zafeirios Fountas¹, Martin A Benfeghoul¹, Adnan Oomerjee¹*, Fenia Christopoulou, Gerasimos Lampouras¹, Haitham Bou-Ammar¹,² and Jun Wang²

¹Huawei Noah’s Ark Lab, London, UK
²AI Centre, Department of Computer Science, University College London, London, UK
{zafeirios.fountas, adnan.ebrahim.oomerjee}@huawei.com
{gerasimos.lampouras, haitham.ammar}@huawei.com
martin.antoine.benfeghoul@h-partners.com
jun.wang@ucl.ac.uk

### ABSTRACT

Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient, human-inspired access to relevant information. Experiments on the LongBench and -Bench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 10 million tokens—a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting parallels between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.

---

### 1 INTRODUCTION

For contemporary pre-trained large language models (LLMs), the context window serves as the primary mechanism to incorporate domain-specific, private, or common up-to-date information. However, despite their remarkable and ever-expanding capabilities, LLMs still exhibit significant limitations when tasked with processing extensive contexts (Liu et al., 2024a). These limitations stem from inherent challenges in Transformer-based architectures. Recent studies have shown that Transformers struggle with extrapolating to contexts longer than their training window size (Kazemnejad et al., 2024). On top of this, employing softmax attention over extended token sequences requires substantial computational resources for each token generation, while the resulting aggregated embeddings (the weighted sums of value vectors) risk becoming excessively noisy and losing their distinctiveness (Tworkowski et al., 2023).

To mitigate these challenges, recent works have focused on retrieval-based methods, either in the form of in-context augmentation (e.g., retrieval-augmented generation (RAG)-based techniques (Lewis et al., 2020; Gao et al., 2024)) or via retrieval of previously-inferred key-value pairs (KV) within individual attention heads (Wu et al., 2022; Tworkowski et al., 2023; Bertsch et al., 2023). Notably, state-of-the-art (SOTA) performance is achieved when KV pairs are initially organised into non-overlapping segments and then retrieved together as one block of sequential tokens (Xiao et al., 2024a).

* Equal Contribution. Code available at: [https://github.com/em-llm/EM-LLM-model](https://github.com/em-llm/EM-LLM-model)

---

**Figure 1: Top: EM-LLMs (surprise only) vs. RAG (NV-Embed-v2 retriever) vs. full-context, with LLaMA-3.1-8B as the base LLM, evaluated on LongBench. Bottom: Comparison of various long-sequence methods (sorted based on their context window length) on an extended version of -Bench's Retrieve.PassKey. Baseline data taken from Ding et al. (2024).**

While such techniques present interesting research avenues, we still see a significant gap between the performance of LLMs in short- vs long-context tasks, even when existing long-context architectures are employed (Liu et al., 2024a). This work tackles the above challenges and attempts to bridge this performance gap by taking inspiration from the algorithmic interpretation of episodic memory in the human brain — the memory system responsible for encoding, storing, and retrieving personal experiences and events. The brain makes sense of its continuous experience in the real world by segmenting it into discrete episodic events (Clewett et al., 2019; Zacks, 2020), which are first organised in a hierarchical and nested-timescale structure (Baldassano et al., 2017) and then stored in long-term memory. Notably, the boundaries between such events are the access points for memory retrieval (Michelmann et al., 2023a) and are widely believed to correspond to points in time with high prediction errors between the brain's generative model and its raw sensory input (a.k.a., surprise). In this context, surprise refers to moments when the brain's predictions about incoming sensory information are significantly violated, leading to a mismatch between what is expected and what is actually perceived. These instances of high surprise are thought to signal important changes in the environment or narrative, prompting the brain to segment the ongoing experience into distinct events (Zacks et al., 2007; 2011; Roseboom et al., 2019; Sinclair et al., 2021; Fountas et al., 2022). Once segmented and stored, the brain recalls episodic memories based on their similarity to current experience, recency, original temporal order, and their proximity to other recalled memories (temporal asymmetry and contiguity (Howard and Kahana, 2002)).

**Contributions:** We propose EM-LLM, a novel architecture integrating crucial aspects of event cognition and episodic memory into Transformer-based LLMs through three key innovations (a, b and c). For memory formation, we segment input token sequences into memory units representing episodic events. The boundaries of these units are (a) initially determined using the model's surprise level during inference, then (b) refined to maximize within-unit cohesion and cross-unit separation (see Section 3.2). This refinement leverages graph-theoretic metrics, treating attention key similarity as a weighted adjacency matrix, and aims to enhance efficient information recall in complex, long-context tasks: by consolidating related information into single units, we seek to minimize the number of units needed for event-specific recall. The resulting memory formation process is computationally efficient: surprise-based segmentation requires no additional computation, and refinement complexity is , where  is typically negligible compared to the token count  in long-context tasks. For memory recall, (c) our approach combines similarity-based retrieval with temporal contiguity and asymmetry mechanisms, building on recently discovered parallels between LLMs and human sequential information retrieval patterns (Ji-An et al., 2024). This method therefore ensures efficient information access while replicating temporal dynamics from human free recall studies (Howard and Kahana, 2002), and enhancing performance on tasks requiring temporal reasoning. See Appendix E.2 for analysis of EM-LLM's architectural contributions.

**Performance:** We show that our method is scalable and significantly outperforms the SOTA retrieval model InfLLM (Xiao et al., 2024a), as well as RAG and full-context methods, on the widely-used LongBench (Bai et al., 2023) and -Bench (Zhang et al., 2024) benchmarks designed for long-context tasks (see Fig. 1). Furthermore, we perform successful passkey retrieval across 10M tokens, a length which is computationally infeasible for current full-context models. To further prove our hypotheses, we then employ a series of human-annotated podcast scripts to show that information in LLM attention heads can be semantically grouped in a way that correlates with the event structure perceived by humans. Therefore, LLM-perceived surprise can indeed serve as a proxy for the cognitive signals that drive human event segmentation, as confirmed by previous studies (Kumar et al., 2023). Finally, using the long-context PG-19 dataset (Rae et al., 2020), which comprises a diverse corpus of English books, we evaluate the effectiveness of our segmentation method for grouping relevant information and assess the performance of different boundary refinement objectives.

### 2 RELATED WORK

#### 2.1 LONG-CONTEXT IN LLMS

Recently, several approaches have been proposed to extend the context window of Transformer-based models. These include methods that address the limited representational capacity of softmax attention, and its quadratic computational and memory cost (Katharopoulos et al., 2020; Munkhdalai et al., 2024). Other methods target the poor extrapolation of typical positional encodings to out-of-distribution context lengths (Kazemnejad et al., 2024). The latter is evident in most widely used methods, including the original absolute positional encodings (Vaswani et al., 2017) and the more recent relative positional encodings, such as the Rotary Positional Embeddings (RoPE) (Su et al., 2024). To address this, some methods propose scaling of the rotation angles (Chen et al., 2024a) or the base constant in ROPE (Xiong et al., 2023; Liu et al., 2024b; Peng et al., 2024; Ding et al., 2024). Others, scale positions without affecting the embedding function (Press et al., 2021; Chen et al., 2023; Jin et al., 2024), explore alternative strategies such as KERPLE (Chi et al., 2022) and FIRE (Li et al., 2024a) or adopt relative position mechanisms from certain LMs like T5 (Raffel et al., 2020).

Concerning computational efficiency and diluted attention, successful approaches propose methods for general improvements to Transformer efficiency through optimised computations (Dao, 2024; Han et al., 2024a; Aminabadi et al., 2022; Kwon et al., 2023; Liu et al., 2024c; Brandon et al., 2023) or compression techniques (Nawrot et al., 2024; Zhang et al., 2023), as well as training methods tailored for long-context scenarios (Zhu et al., 2024; Chen et al., 2024b). Another direction is the utilisation of retrieval-based methods, the vast majority of which relies on a vector database that keeps a key-value cache and scalable approximations of k-nearest neighbors (k-NNs) to perform lookups (Wu et al., 2022; Tworkowski et al., 2023; Bertsch et al., 2023). Interestingly, since using a key-value cache with k-NN lookup can be seen as an approximation of applying softmax attention to the full token sequence (see Appendix F.1), k-NN retrieval methods can be used without fine-tuning (Bertsch et al., 2023). For an exception that does not rely on k-NNs, see Wang et al. (2023).

A recent and interesting variant of k-NN retrieval involves retrieving large groups of tokens, rather than individual ones. Models that rely on this approach include SLED (Ivgi et al., 2023) and the more recent InfLLM (Xiao et al., 2024a), which achieves SOTA performance on long-context benchmarks. InfLLM segments the entire context length into fixed-size memory units and employs k-NN lookup using the tokens with the highest accumulated scores per unit. The latter can be seen as a form of hierarchical attention in models that use such retrieval, as illustrated in Fig. 2. While group-based retrieval represents a promising direction, our approach significantly advances this concept by dynamically determining token groupings in a manner akin to human memory formation. This effectively addresses a fundamental limitation of InfLLM's fixed-size segmentation and enables more adaptive and context-sensitive processing of extended information.

#### 2.2 NEURAL MODELS OF EPISODIC MEMORY AND EVENT COGNITION

The concept of episodic memory, central to our approach, has been extensively studied in both theoretical neuroscience and machine learning. Neural models of episodic memory capture human behaviour and neuroimaging data, providing insights into how the brain processes and stores experiences and suggesting links between memory, efficient representations and navigation of physical and conceptual spaces (Gershman et al., 2012; Benna and Fusi, 2021). In machine learning, episodic memory-inspired approaches have yielded significant improvements across various domains. For instance, episodic control has enhanced reinforcement learning agents performance and learning speed (Blundell et al., 2016; Pritzel et al., 2017; Coda-Forno et al., 2024). In addition, models of memory construction and consolidation have been successful in alleviating catastrophic forgetting in neural networks (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019; Buzzega et al., 2020; Prabhu et al., 2020), including LLMs (Das et al., 2024), and appear to explain key features of human memory, such as imagination and future thinking (Spens and Burgess, 2024). These models have revealed key aspects of episodic memory, particularly in describing how experiences are segmented into events, and when new memories are encoded and retrieved (Lu et al., 2022). Surprise plays a critical role in this process, triggering event boundaries and memory formation (Fountas et al., 2022; Kumar et al., 2023). This event-based structure is deeply intertwined with our perception of time (Roseboom et al., 2019; Sherman et al., 2022), highlighting the interdependence of memory and temporal cognition. This insight has helped generative models for video (Zakharov et al., 2022a;b) and reinforcement learning (Zakharov et al., 2021) to capture temporal dynamics more accurately.

In terms of memory retrieval, studies in human free recall have shown a distinctive increased likelihood of retrieving items encoded close together in time (temporal contiguity) and in succession (temporal asymmetry) (see Fig. 3A). Recently, it was shown that attention heads in Transformer-based LLMs that are associated with in-context learning, already exhibit the same dynamic retrieval behaviour (Ji-An et al., 2024) (Fig. 3B) including both contiguity and asymmetry effects. Therefore, Transformers have the inherent ability to act as episodic memory retrieval models, if provided with the right information within their context window. Our work leverages these concepts of surprise-based event segmentation and LLMs' inherent temporal contiguity and asymmetry effects to enable a new generation of Infinite Context-Length LLMs, capable of processing and understanding information over vastly extended timescales.

---

**Figure 2: Group-based k-NN retrieval can be seen as a form of hierarchical episodic attention.** Initially,  groups of tokens are selected (left) and then used for softmax attention (right), as if all other similarity scores were forced to be zero (non-shaded areas of the left curve). This framework can support multiple levels of episodic attention.

### 3 EM-LLM: LLM WITH EPISODIC MEMORY

#### 3.1 ARCHITECTURE

EM-LLM is designed to be applied directly to pre-trained LLMs, enabling them to handle context lengths significantly larger than their original training length. Our architecture, illustrated in Fig. 3C, divides the context into three distinct groups: initial tokens, evicted tokens and local context. This structure, while incorporating insights from recent work on token block retrieval (Xiao et al., 2024a), introduces novel elements inspired by human episodic memory. The local context represents the most recent tokens, maximising information about the current task, and fits within the typical context window of the underlying LLM. This group utilises full softmax attention and plays a role similar to the focus of attention in cognitive models of working memory, holding the most immediately relevant information for the current task (Cowan, 2001). The evicted tokens typically comprise the majority of past tokens in a long-context scenario, extending far beyond the LLM's original training length. These tokens are managed by our proposed memory model functioning similarly to short-term episodic memory in the brain. Finally, following previous work, we also maintain a group of 128 initial tokens in the LLM context. These act as attention sinks and help recover the performance of window attention, as first observed by Xiao et al. (2024b); Han et al. (2024b) and later adopted by Xiao et al. (2024a). For retrieved tokens, which are therefore discontinuous and outside the local context, we assign a fixed position embedding as in Raffel et al. (2020); Xiao et al. (2024a). This architecture enables EM-LLM to effectively process and utilise information from positions outside its pre-trained local context window, while maintaining the underlying LLM's performance characteristics.

#### 3.2 MEMORY FORMATION VIA SURPRISE

In the context of LLMs, we define episodic memory as the organised, event-based collection of past key-value pairs, analogous to the latent representations of personal experiences in human memory. Just as unexpected or novel information plays a crucial role in human memory formation, we posit that analogous indicators of novelty in LLMs can serve as an effective proxy for identifying significant "events" within the model's experience. In Bayesian terms, surprise is quantified by the negative log-likelihood of observing the current, ground-truth token given the previous tokens in an auto-regressive model, with high values indicating the unpredictability or novelty of each new token within the context according to the model, i.e., being "surprised" by the next token. Following work on cognitive modelling (Roseboom et al., 2019; Fountas et al., 2022), we employ a thresholding mechanism to perform an initial identification of event boundaries (used for the first time in LLMs). Formally, a token  is considered a potential boundary if its surprise value exceeds a threshold :

where  and  are the mean and variance of surprise for a window offset , and  is a scaling factor. The choice of threshold  is critical in balancing the granularity of segmentation with the model's sensitivity to contextual shifts. If the  is too high, we will identify very few event boundaries, especially if the local context contains few surprising tokens. Conversely, a low  results in frequent boundary identification. Using a moving window ensures that  adapts to contextual shifts, minimizing the need for manual tuning while maintaining control over threshold sensitivity via . This initial segmentation results in a set of potential event boundaries , where each  represents the index of a token exceeding the surprise threshold. These boundaries serve as the starting point for our subsequent refinement process, which aims to optimise the intra-event coherence and inter-event distinctiveness of the resulting memory segments.

---

**Figure 3: (A) Example of the temporal contiguity and asymmetry effect in human free recall.** Data averaged over several large free recall studies (adopted from Howard and Kahana (2002)). **(B) The attention scores of a GPT2 head averaged over all tokens tested (adopted from Ji-An et al. (2024)). (C) Schematic illustrating our proposed process for memory formation and retrieval in each layer:** ① Input sequence with surprise-based segmentation (purple arrows indicate high surprise). ② Formation of episodic memories: input is segmented into events and stored, with initial tokens and local context preserved. Note that the boundary refinement process is not shown here for clarity. ③ Memory retrieval via k-NN search, selecting contiguous events from episodic memory. ④ Final context window structure, comprising initial tokens, contiguity buffer (populated by neighbouring events), similarity buffer (from k-NN retrieval), and local context.

#### 3.3 BOUNDARY REFINEMENT

While surprise-based segmentation provides an effective initial estimate of event boundaries, we make the key observation that the utility of elements within an event, during memory recall, depends on their likelihood of being utilised by the current query. Therefore, we theorise that memory recall will be most efficient with high intra-event similarity between keys while maintaining low inter-event similarity. For instance, see the similarity of groups in Fig. 2. To further ensure this, we introduce a boundary refinement step that looks to optimise this objective. Such an objective is typically optimised in the context of graph-clustering, hence we express this refinement process in a graph-theoretic manner. To achieve this, we treat the similarity matrix between all keys of an attention head  within the local context window for tokens  as an adjacency matrix :

where  and  are the key vectors corresponding to tokens  and , respectively. The similarity function measures the closeness of two key vectors; in our implementation, we use dot product similarity  due to its effectiveness in capturing semantic relationships in high-dimensional spaces (Vaswani et al., 2017) and to align with the mechanism of self-attention in Transformers.

To evaluate the quality of potential boundaries, we define a metric function . This function quantifies the cohesion within events and separation between events based on the graph structure represented by the similarity matrix  and event boundaries . We experiment with two widely-accepted graph-clustering metrics: modularity and conductance (Miasnikof et al., 2018). Modularity (Newman and Girvan, 2004) provides a measure of the quality of a particular division of a network into communities, with higher values indicating higher edge density in the identified cluster when compared to the density of edges expected in a random cluster. As our edge weights represent the similarity between two tokens, we seek to maximise this metric. Modularity is defined as:

where  is the total edge weight in the graph,  is the community (episodic event) to which node  is assigned, and  is the Kronecker delta function. Conductance, on the other hand, measures the fraction of total weighted edges cut by a given community boundary, and is defined as:

where  is a subset of all nodes  in the induced graph, with . Lower conductance values indicate better community structure. Our boundary refinement algorithm sequentially adjusts the initial surprise-based boundaries to optimise these metric functions. While our best results are achieved using modularity, we also include comparisons with conductance-based boundary refinement to provide a comprehensive analysis. The overall process is summarized in Algorithm 1 and further discussed in Appendix E.3. This algorithm first identifies initial boundaries based on the surprise threshold , then refines these boundaries by finding the optimal position  between each pair of consecutive initial boundaries (, ) that optimises the chosen metric function  (either maximising modularity or minimising conductance). This process ensures that the final segmentation (1) captures points of high surprise and (2) optimises for coherent information grouping. The boundary identification step incurs negligible computational cost, as it only evaluates existing LLM outputs. The time complexity of Algorithm 1 has an overall complexity of , where  is the sequence length and  is the chunk size selected to process the sequence (for details see Appendix C.1).

---

**Algorithm 1** Event segmentation in KV cache
**Input:** tok: List of tokens in the sequence
**Input:** : Threshold for surprisal to identify initial boundaries
**Input:** : Metric function to evaluate potential boundaries
1:   Boundary identification
2: **for**  **in** range(length() - 1) **do**  Boundary refinement
3: \quad 
4: \quad 
5: **end for**
6: **return** 

---

| Base LLM | Method | SQA | LongBench MQA Sum FSL | Ret | Cod | Avg. | -Bench C.D | M.F | MC R.KV | R.P | R.N |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Mistral v2 | InfLLM (4k+2k) | 33 | 25.5 27.1 66.1 | 64 | 54.8 | 41.9 | 29.4 | 26.6 | 95.6 43.2 | 100 | 99.8 |
|  | EM-LLM$_{SM+C}$ | 32.9 | 27 27.2 66.8 | 84.1 | 54.8 | 43.7 | 28.2 | 27.1 | 99 42.8 | 100 | 99.8 |
| LLaMA 3 | InfLLM (4k+4k) | 38.5 | 36.9 27.0 69.0 | 84.0 | 53.2 | 47.0 | 30.5 | 23.7 | 43.7 5.0 | 100 | 99.0 |
|  | EM-LLM$_{S}$ | 39.3 | 37.7 27.2 69.2 | 87.5 | 50.3 | 47.2 | 31.7 | 16.9 | 40.6 4.2 | 100 | 99.5 |
| LLaMA 3.1 | InfLLM (4k+4k) | 41.4 | 40.7 29.0 69.0 | 97.0 | 64.2 | 51.1 | 22.6 | 33.7 | 46.7 81.0 | 100 | 100 |
|  | EM-LLM$_{SM}$ | 41.2 | 41.3 29.2 69.1 | 98.5 | 64.1 | 51.3 | 22.6 | 34.0 | 47.6 90.2 | 100 | 100 |
| Phi 3 | InfLLM (1k+3k) | 28.4 | 24.9 25.6 52.9 | 7.5 | 57.0 | 34.5 |  |  |  |  |  |
|  | EM-LLM$_{S}$ | 29.2 | 27.1 25.9 53.5 | 10.0 | 57.0 | 35.4 |  |  |  |  |  |
| Phi 3.5 | InfLLM (1k+3k) | 31.7 | 28.5 23.9 56.3 | 11.5 | 40.3 | 34.2 |  |  |  |  |  |
|  | EM-LLM$_{S}$ | 31.8 | 31.9 24.5 55.5 | 13.0 | 39.5 | 34.9 |  |  |  |  |  |

**Table 1: EM-LLM performance on LongBench (grouped tasks) and -Bench compared to our baseline InfLLM.** S: surprise threshold, SM: surprise threshold and refinement with modularity, S+C: surprise threshold and contiguity buffer, SM+C: surprise, refinement and contiguity buffer. Each row indicates the number of local + retrieved tokens (eg. 4k+2k) used for both InfLLM and EM-LLM. See Appendix D.1 for parameter choices and Appendix A.1 for more results and significance testing.

#### 3.4 MEMORY RETRIEVAL

When inferring a new token, a number of episodic events are selected and become a part of the (extended) context window of the underlying LLM. Our memory retrieval process employs a two-stage mechanism to select relevant episodic events for the LLM's context window (Fig. 3C). First, we retrieve  events using k-NN search based on dot product similarity between the current query and representative tokens of each event. These representatives, selected as per Xiao et al. (2024a), are the most influential tokens within each event. For large memory stores, we utilise approximate k-NN (Douze et al., 2024) to maintain efficiency. These  events, retrieved based on their similarity to the current query, form a part of the LLM's context window that we refer to as the similarity buffer.

The second stage of our retrieval process introduces another buffer, which we refer to as the contiguity buffer, designed to maintain temporal context. Implemented as a queue of size , this buffer promotes temporal relationships in retrieval. When an event is retrieved, we also enqueue its neighboring events (within  positions in the original sequence) into this buffer. This mechanism enables the LLM's "induction" attention heads to exhibit the contiguity and asymmetry effects discussed in Section 2.2. The queue structure allows for a natural decay of temporal context as new events are processed, with older or repeated events being dequeued as new ones are added. In total,  events are added to the context window, striking a balance between relevance and temporal relationships in a manner analogous to human episodic memory retrieval. Note that each layer retrieves and attends to these  events individually, allowing it to potentially focus on different parts of the context.

### 4 EXPERIMENTS

#### 4.1 PERFORMANCE OF EM-LLM ON LONG-CONTEXT TASKS

**Comparison with KV-retrieval-based LLMs** At the time of writing, InfLLM is considered to be the SOTA KV-retrieval method on long-context benchmarks (LongBench, -Bench), as well as being the only method which uses group-based k-NN retrieval in LLMs on such benchmarks. We, therefore, employ this model as our first baseline for comparison with our own methods. Results on both benchmarks (Table 1) show that our method is able to improve on InfLLM across 5 different base LLMs, 80% of individual task groups of LongBench and on the overall average. Note that the table shows the best single method in terms of overall performance for each ablation (see Appendix A.1 for all ablations in methods). Looking at individual task performance across all ablations in methods, EM-LLM is able to surpass InfLLM in all tasks. Notably, we see an especially large jump in performance in the retrieval (Passage, KV, Passkey, Number) and QA (Narrative, Qasper, MultiField, Hotpot, 2Wiki and Musique) tasks across all ablations, with up to a 40% and 29.7% improvement over InfLLM respectively. Such tasks require the model to identify and retrieve specific information within the input sequence, a challenging test for the model's ability to accurately recall a wide range of detailed information from a large context concurrently. This substantial improvement highlights the effectiveness of our event segmentation method in enhancing long-term memory recall and retrieval accuracy in LLMs.

---

**Figure 4: Comparison of human event segmentation with different computational segmentation methods in a human-annotated audio dataset (see also Appendix B).** (A) Difference in metrics for the cohesion and separation of KV cache of each LLaMA2 layer. The graphs report the difference of each method with the corresponding random segmentation. (B) Distance between human reports and different methods. In both sets of results, fixed methods (F, FM, FC | with M: Modularity, C: Conductance) perform worse than their surprise-based counterparts (S, SM, SC) with InfLLM's method (F) performing worse than random.

**Comparison with RAG and full-context LLMs** To evaluate EM-LLM against prominent methods for handling long contexts, we compared its performance on LLaMA 3.1-8B with two different RAG approaches, including the current SOTA NV-Embed-v2 retriever (Lee et al., 2024), as well as with the brute-force baseline of processing all tokens directly within the LLM's softmax attention (full-context). Across most tasks in our benchmarks, EM-LLM outperformed both RAG and full-context methods, as well as a custom surprise-based RAG method (Fig. 1 and Appendix A.2), exceeding the performance of NV-Embed-v2 by 30.5% on LongBench and by 11.5% on -Bench. This significant performance boost over RAG can be attributed to EM-LLM's ability to retrieve and incorporate relevant information at each layer individually, rather than relying on a single retrieval step as in RAG (for an illustration, see Supp. Fig. 5). By accessing more specific and contextually relevant information through layer-wise key-value retrieval, EM-LLM effectively addresses RAG's limitations in precision and lower overall performance (Li et al., 2024b). Additionally, EM-LLM's hierarchical attention avoids the issue of diluted attention in large context windows that affects full-context models, enabling it to outperform both RAG and full-context LLMs on the LongBench dataset. Furthermore, EM-LLM demonstrated remarkable scalability by achieving 100% accuracy on the Passkey.Retrieval task with sequences up to 10.2M tokens, far beyond the practical limits of full-context LLMs. This highlights EM-LLM's efficiency in handling extremely long contexts, positioning it as a powerful alternative for long-context processing.

#### 4.2 HUMAN AND LLM SURPRISE CLUSTER SIMILAR TOKENS TOGETHER

As mentioned in Section 3.2, we employ modularity and conductance as two refinement objectives in our boundary refinement algorithm, due to their qualities in assessing the intra- and inter-event similarities between individual tokens. We will now use such metrics to compare various event segmentation methods, including human event segmentation data. Additionally, we introduce one further, simple metric for this experiment: the ratio between intra- and inter-community similarity () calculated for each head and community  as follows:

Kumar et al. (2023) found strong correlations between human-perceived events and prediction errors across 3 short podcasts (7-30 minutes), when processing the corresponding transcript with an LLM. Taking advantage of such human-annotated data and results from previous studies on this dataset (Michelmann et al., 2021; Lositsky et al., 2016), we compare the segmentation quality and correlation with human segmentation for each of our methods (Fig. 4) using our similarity metrics. As shown in Fig. 4A, human-perceived events achieve significantly higher scores in similarity metrics compared to fixed or random events, suggesting that surprise is indeed an important factor for humans in their own perception of events. Furthermore, surprise-only segmentation (S) achieves very similar results to humans, while the addition of our refinement algorithm (SM, SC, FM, FC) significantly improves performance. Fig. 4B further shows that surprise-based methods (S, SM, SC), consistently identify event boundaries that are closest to those perceived by humans.

---

| LLM | Metric | F | FM | FC | S | SM | SC |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Mistral-7B | Mod |  |  |  |  |  |  |
|  | Con |  |  |  |  |  |  |
|  | I/IS |  |  |  |  |  |  |
| LLaMA2-7B | Mod↑ |  |  |  |  |  |  |
|  | Con |  |  |  |  |  |  |
|  | I/IS |  |  |  |  |  |  |
| LLaMA3-8B | Mod |  |  |  |  |  |  |
|  | Con |  |  |  |  |  |  |
|  | I/IS |  |  |  |  |  |  |

**Table 2: Comparison with graph-theoretic metrics in the KV cache of different LLMs and segmentation methods using the PG-19 dataset and .** Reported values are the difference with random segmentation. Mod: modularity , Con: conductance, I/IS: intra/inter-similarity .

#### 4.3 COMPARING SEGMENTATION METHODS

Our experiments on the PG-19 dataset (see Table 2) clearly demonstrate that surprise-based segmentation with refinement (SM, SC) provides the best results in terms of event similarity metrics, regardless of the base LLM used. While the surprise-only method (S) achieves decent results, we observe that refinement is especially adept to improving this performance with regards to our metrics, as it is directly optimising for such an objective. Interestingly however, the fixed-based refinement methods (FM, FC) do not reach the same performance as their surprise-based counterparts, further showing that the initial segmentation with a surprise threshold is crucial to achieving the best possible balance in intra-/inter-similarity with our methods.

#### 4.4 SIMILARITY, CONTIGUITY, RECENCY AND TEMPORAL ORDER

As demonstrated in Tables 1 and 2, along with Fig. 4, each of our ablations show various positive improvements on InfLLM. As mentioned in Section 4.3, refinement has a strong positive impact in improving our similarity metrics. This is seen to translate well to model performance in our experiments, with the addition of refinement achieving the best performance in 60% of tasks across LongBench and -Bench (see Tables 3-7), as well as agreeing with human data (Fig. 4). The effects of contiguity are also clearly demonstrated, with the addition of our contiguity buffer achieving the best performance in 44% of tasks. Furthermore, these methods are seen to be complementary, often improving on both individual additions. However, the fact that certain tasks still appear to benefit more from either surprise-only, refinement, or contiguity, is an interesting result. This is likely due to the nature of the tasks and the varying importance of contiguity across these tasks. Where contiguity is not crucial, adding such a buffer to our context window also reduces the size of the similarity buffer, and therefore provides potentially less directly relevant events. This is compatible with our own findings that a contiguity buffer that is as big or smaller than the similarity buffer yields the best results (see Fig. 13), suggesting that the similarity buffer is still the most crucial part of our approach. This is especially the case when combined with refinement, which we expect is due to the improved similarity of refined events, hence further reducing the need for contiguous events.

### 5 DISCUSSION

**Human studies** Significant correlations have been found between human event segmentation and prediction errors in both LLMs (Kumar et al., 2023) and video models (Fountas et al., 2022; Mariola et al., 2022). Our results add to this growing body of evidence, demonstrating that LLM-based surprise can serve as a proxy for human event segmentation, in multiple levels of hierarchical abstraction, and that the resulting event structure in EM-LLM's attention heads correlates strongly with human-perceived events. This finding suggests a potential, low-level parallels between LLM mechanisms and human cognitive processes (see also Appendix E.1). Furthermore, our model's use of both similarity-based and temporally contiguous retrieval mechanisms parallels human memory retrieval patterns, allowing for the expression of robust phenomena found in human memory research (Howard and Kahana, 2002). The temporal contiguity effect, where items experienced close together in time are often recalled together, is a robust phenomenon in human memory research (Howard and Kahana, 2002). Further experiments could deepen our understanding of the connections between EM-LLM and human episodic memory. Following Michelmann et al. (2023b), one could test whether the timing of the event boundaries or the degree of modularity per level that our method produces is closer on average to the human consensus, than individual human subjects. Additionally, exploring how different ratios of the contiguity buffer affect the reproduction of human memory biases, and investigating the impact of recency and initial surprise on event recall, could reveal the extent to which EM-LLM exhibits biases found in free recall studies.

Furthermore, EM-LLM's architecture with differentiated context handling (Section 3.1) invites comparisons to cognitive models of human memory beyond episodic. The local context, holding recent and task-relevant information, resembles the limited-capacity working memory system described by Baddeley (2003). Given that EM-LLM's broader context window includes both local context and retrieved memories, it aligns more closely with Ericsson and Kintsch (1995)'s concept of long-term working memory, which allows rapid access to relevant long-term information beyond traditional capacity limits. Alternatively, our architecture parallels Cowan (2001)'s embedded-processes model, where the local context is the "focus of attention", and the full context window represents the activated portion of long-term memory. Future work could explore these analogies further, using EM-LLM as a test-bed for hypotheses about human memory and working memory capacity limits. Inspired by Baddeley's multi-component model, integrating modality-specific buffers into EM-LLM might enhance performance on multi-modal tasks.

**Machine learning** In refining event boundaries, we utilised modularity and conductance as metrics for evaluating community structure in the similarity graph of attention keys. While effective in our experiments, we acknowledge that numerous other methods for graph clustering and sequence segmentation could potentially be applied (Fortunato, 2010; Yang et al., 2016). Our choice was motivated by their established theoretical foundations and computational efficiency, though comparative studies suggest performance can vary based on network characteristics (Yang et al., 2016). Interestingly, our surprise-based initial boundary detection shares similarities with Bayesian online change-point detection (Adams and MacKay, 2007), suggesting potential avenues for integrating time series analysis techniques into LLM context processing. Future work could explore whether more sophisticated segmentation or clustering algorithms could improve EM-LLM's performance, particularly for extremely long contexts or streaming data scenarios. Such investigations could enhance our model and contribute to understanding how information is structured and processed in LLMs, bridging the gap between traditional sequence analysis and LLM context processing. Looking ahead, promising directions for future research include extending our segmentation processes to operate at each layer of the Transformer independently. This could lead to more nuanced and hierarchical representations of episodic memories, following the underlying semantic structure of the input more closely. Additionally, exploring how EM-LLM could be utilised to enable imagination and future thinking has great potential for advancing model-based reinforcement learning and continual learning techniques in LLMs. By leveraging its event-based structure to simulate potential future scenarios or recall past experiences in novel contexts, EM-LLM could enhance an LLM's ability to plan, adapt, and learn continuously from new information.

### 6 CONCLUSION

In this work, we introduced EM-LLM, a flexible architecture that integrates key aspects of human episodic memory and event cognition into Transformer-based LLMs. Our approach enables existing LLMs to effectively process vastly extended contexts without the need for pre-training, demonstrating superior performance on long-context tasks compared to the corresponding SOTA. By combining surprise-based event segmentation, graph-theoretic boundary refinement, and a two-stage memory retrieval process, EM-LLM offers a promising path toward virtually infinite context windows. This capability has the potential to revolutionize interactions with LLMs, enabling continuous, personalised exchanges over extended periods and serving as a viable alternative to traditional RAG techniques. Finally, by bridging insights from cognitive science with machine learning, our approach not only enhances the performance of LLMs on long-context tasks but also provides a scalable framework for computational modelling of episodic and event cognition. We hope this study inspires the community to expand research at the intersection of LLMs and human memory.